<html>
  <head>
    <title>
      PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts
    </title>
    <link
      rel="icon"
      href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 32 32%22><text y=%2226%22 font-size=%2232%22>ðŸ¦œ</text></svg>"
    />
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="stylesheet" href="./css/index.css" />
    <script src="./js/showBibtex.js" defer></script>
  </head>

  <body>
    <div class="header">
      <div class="paper-title">
        <a href="#"
          >
          <br />
	  PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts</a
        >
      </div>
      <div>
        <!-- * Paper -->
        <span class="link-block">
          <a target="_blank" href="https://arxiv.org/abs/2502.08946"
            ><img
              class="badge"
              src="https://img.shields.io/badge/Paper-PDF-red?&labelColor=lightgray&logo=arxiv&logoColor=brown&style=flat-square" /><img
          /></a>
        </span>
        <!-- * Dataset -->
        <span class="link-block">
          <a
            target="_blank"
            href="https://huggingface.co/datasets/ShunchiZhang/PhysiCo/"
            ><img
              class="badge"
              src="https://img.shields.io/badge/HuggingFace-Dataset-yellow?labelColor=lightgray&logo=huggingface&style=flat-square" /><img
          /></a>
        </span>
        <!-- * Leaderboard -->
        <span class="link-block">
          <a href="./leaderboard.html"
            ><img
              class="badge"
              src="https://img.shields.io/badge/Leaderboard-Visit-blue?labelColor=lightgray&logo=stackblitz&logoColor=367BAF&style=flat-square" /><img /></a
        ></span>
        <!-- * Code -->
        <span class="link-block">
          <a target="_blank" href="https://github.com/physico-benchmark/physico"
            ><img
              class="badge"
              src="https://img.shields.io/badge/GitHub-Code-gray?labelColor=lightgray&logo=github&logoColor=gray&style=flat-square" /><img /></a
        ></span>
        <!-- * X Threads -->
        <span class="link-block">
          <a href="https://x.com/jieeijjie/status/1876713099752861782"
            ><img
              class="badge"
              src="https://img.shields.io/badge/Threads-black?labelColor=lightgray&logo=x&logoColor=gray&style=flat-square" /><img /></a
        ></span>
        <!-- * BibTeX -->
        <span class="link-block">
          <a class="link-block" href="./cite.bib" id="bibtex-link">
            <img
              class="badge"
              src="https://img.shields.io/badge/BibTeX-Cite-4285F4?labelColor=lightgray&logo=googlescholar&logoColor=3579EC&style=flat-square"
            />
          </a>
        </span>
      </div>
      <div>
        <div class="author-list">
          <span class="author-block"
            ><a href="https://sites.google.com/site/moyunlp/">Mo Yu</a
            ><sup>1=</sup></span
          >
          <span class="author-block"
            ><a href="https://ttchungc.github.io/">Tsz Ting Chung</a
            ><sup>2=</sup></span
          >
          <span class="author-block"
            >Chulun Zhou
            <sup>3=</sup></span
          >
          <span class="author-block"
            >Tong Li
            <sup>3=</sup></span
          >
          <span class="author-block"
            >Rui Lu
            <sup>3=</sup></span
          >
          <span class="author-block"
            >Jiangnan Li
            <sup>1=</sup></span
          >
          <span class="author-block"
            >Liyan Xu
            <sup>1=</sup></span
          >
        </div>
        <div class="author-list">
          <span class="author-block"
            >Haoshu Lu
            <sup>4</sup></span
          >
          <span class="author-block"
            >Ning Zhang
            <sup>1</sup></span
          >
          <span class="author-block"
            >Jing Li
            <sup>4</sup></span
          >
          <span class="author-block"
            >Jie Zhou
            <sup>1</sup></span
          >
        </div>
        <div class="inst-list">
          <span class="institution-block"><sup>1</sup> WeChat AI</span>
          <span class="institution-block"><sup>2</sup> HKUST</span>
          <span class="institution-block"><sup>3</sup> CUHK</span>
          <span class="institution-block"><sup>3</sup> NJIT</span>
        </div>
        <div class="note-block"><sup>=</sup> equal contribution</div>
        <div class="venue-block">Spoiler alert: We show it is possible to measure <i>Fluid Intelligence</i> in natural language space.</div>
      </div>
    </div>

    <div class="teaser">
      <center>
        <img src="./img/teaser.png" />
        <figcaption>
          Illustration of two examples from by our
          <span class="dataset-name">PRELUDE</span> task 
          that are <i>Consistent</i> and <i>Contradict</i> to the canonical book, respectively.
        </figcaption>
      </center>
    </div>

    <hr />

    <!-- ^ Summary of Our Research -->
    <center><h1>Summary of Our Research</h1></center>
    <div class="content">
      <!-- <p style="font-size: large; text-align: center">
        <b>TL;DR:</b>
      </p> -->
      <p>
	We introduce PRELUDE, a benchmark for evaluating long-context understanding through the task of determining whether a character's prequel story is consistent with the canonical narrative of the original book.
	Our task poses a stronger demand for global comprehension and deep reasoning than existing benchmarks -- as the prequels are not part of the original story, assessing their plausibility typically requires searching and integrating information that is only indirectly related. Empirically, 88% of instances require evidence from multiple parts of the narrative.

	Experimental results highlight the challenge of our task: in-context learning, RAG and in-domain training with state-of-the-art LLMs, and commercial DeepResearch services, lag behind humans by >15%. 
	A further human study reveals that models often produce correct answers with flawed reasoning, leading to an over 30% gap in reasoning accuracy compared to humans.
	These findings underscore the substantial room for improvement in long-context understanding and reasoning. 
      </p>
      <ol>
        <li>
          state-of-the-art LLMs, including GPT-4o, o1 and Gemini 2.0 flash
          thinking, lag behind humans by ~40%;
          <a href="./leaderboard.html"
            ><img
              style="vertical-align: middle"
              src="https://img.shields.io/badge/Leaderboard-Visit-blue?labelColor=lightgray&logo=stackblitz&logoColor=367BAF&style=social" /><img
          /></a>
        </li>
        <li>
          the stochastic parrot phenomenon is present in LLMs, as they fail on
          our grid task but can describe and recognize the same concepts well in
          natural language;
        </li>
        <li>
          our task challenges the LLMs due to intrinsic difficulties rather than
          the unfamiliar grid format, as in-context learning and fine-tuning on
          same formatted data added little to their performance.
        </li>
      </ol>
    </div>

    <!-- ^ Comparison -->
    <center><h1>Comparison to Existing Story Understanding Benchmarks</h1></center>
    <div class="content">
      <!-- <p>Foo Bar</p> -->
      <center>
        <img src="./img/rq.png" width="70%" />
        <!-- <figcaption style="text-align: center">
          6 Research Questions (RQs) and 3 Hypotheses
        </figcaption> -->
      </center>
    </div>
  </body>
</html>
