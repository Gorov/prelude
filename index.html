<html>
  <head>
    <title>
      PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts
    </title>
    <link
      rel="icon"
      href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 32 32%22><text y=%2226%22 font-size=%2232%22>üìù</text></svg>"
    />
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="stylesheet" href="./css/index.css" />
    <script src="./js/showBibtex.js" defer></script>
  </head>

  <body>
    <div class="header">
      <div class="paper-title">
        <a href="#"
          >
          <br />
	  <span class="dataset-name">PRELUDE</span>: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts</a
        >
      </div>
      <div>
        <!-- * Paper -->
        <span class="link-block">
          <a target="_blank" href="/prelude/assets/pdfs/paper.pdf"
            ><img
              class="badge"
              src="https://img.shields.io/badge/Paper-PDF-red?&labelColor=lightgray&logo=arxiv&logoColor=brown&style=flat-square" /><img
          /></a>
        </span>
        <!-- * Dataset -->
        <span class="link-block">
          <a
            target="_blank"
            href="https://huggingface.co/datasets/ttchungc/PRELUDE"
            ><img
              class="badge"
              src="https://img.shields.io/badge/HuggingFace-Dataset-yellow?labelColor=lightgray&logo=huggingface&style=flat-square" /><img
          /></a>
        </span>
        <!-- * Leaderboard -->
        <span class="link-block">
          <a href="./leaderboard.html"
            ><img
              class="badge"
              src="https://img.shields.io/badge/Leaderboard-Visit-blue?labelColor=lightgray&logo=stackblitz&logoColor=367BAF&style=flat-square" /><img /></a
        ></span>
        <!-- * Code -->
        <!-- <span class="link-block">
          <a target="_blank" href="https://github.com/physico-benchmark/physico"
            ><img
              class="badge"
              src="https://img.shields.io/badge/GitHub-Code-gray?labelColor=lightgray&logo=github&logoColor=gray&style=flat-square" /><img /></a
        ></span> -->
        <!-- * X Threads -->
        <!-- <span class="link-block">
          <a href="https://x.com/jieeijjie/status/1876713099752861782"
            ><img
              class="badge"
              src="https://img.shields.io/badge/Threads-black?labelColor=lightgray&logo=x&logoColor=gray&style=flat-square" /><img /></a
        ></span> -->
        <!-- * BibTeX -->
        <!-- <span class="link-block">
          <a class="link-block" href="./cite.bib" id="bibtex-link">
            <img
              class="badge"
              src="https://img.shields.io/badge/BibTeX-Cite-4285F4?labelColor=lightgray&logo=googlescholar&logoColor=3579EC&style=flat-square"
            />
          </a>
        </span> -->
      </div>
      <div>
        <div class="author-list">
          <span class="author-block"
          <span class="author-block"
            ><a href="https://sites.google.com/site/moyunlp/">Mo Yu</a
            ><sup>1=</sup></span
          >
          <span class="author-block"
            ><a href="https://ttchungc.github.io/">Tsz Ting Chung</a
            ><sup>2=</sup></span
          >
          <span class="author-block"
            ><a href="https://encyclomen.github.io/">Chulun Zhou</a><sup>3=</sup></span
          >
          <span class="author-block"
            ><a href="https://openreview.net/profile?id=~Tong_Li32">Tong Li</a><sup>3=</sup></span
          >
          <span class="author-block"
            ><a href="https://openreview.net/profile?id=~Rui_Lu9">Rui Lu</a><sup>3=</sup></span
          > 
          <span class="author-block"
            ><a href="https://openreview.net/profile?id=~Jiangnan_Li2">Jiangnan Li</a><sup>1=</sup></span
          >
          <span class="author-block"
            ><a href="https://lxucs.github.io/">Liyan Xu</a><sup>1=</sup></span
          >
        </div>
        <div class="author-list">
          <span class="author-block"
            ><a href="https://openreview.net/profile?id=~Haoshu_Lu1">Haoshu Lu</a><sup>4</sup></span
          >
          <span class="author-block"
            ><a href="https://openreview.net/profile?id=~jonahnzhang1">Ning Zhang</a><sup>1</sup></span
          >
          <span class="author-block"
            ><a href="https://web.njit.edu/~jingli/">Jing Li</a><sup>4</sup></span
          >
          <span class="author-block"
            ><a href="https://openreview.net/profile?id=~Jie_Zhou8">Jie Zhou</a><sup>1</sup></span
          >
        </div>
        <div class="inst-list">
          <span class="institution-block"><sup>1</sup> WeChat AI</span>
          <span class="institution-block"><sup>2</sup> HKUST</span>
          <span class="institution-block"><sup>3</sup> CUHK</span>
          <span class="institution-block"><sup>4</sup> NJIT</span>
        </div>
        <div class="note-block"><sup>=</sup> equal contribution</div>
        <div class="venue-block">Spoiler alert: We show it is possible to measure <i>Fluid Intelligence</i> in natural language space.</div>
      </div>
    </div>

    <div class="teaser">
      <center>
        <img src="./img/teaser.png" />
        <figcaption>
          Illustration of two examples from by our
          <span class="dataset-name">PRELUDE</span> task 
          that are <i>Consistent</i> and <i>Contradict</i> to the canonical book, respectively.
        </figcaption>
      </center>
    </div>

    <hr />

    <!-- ^ Summary of Our Research -->
    <center><h1>Summary of Our Research</h1></center>
    <div class="content">
      <!-- <p style="font-size: large; text-align: center">
        <b>TL;DR:</b>
      </p> -->
      <p>
	We introduce PRELUDE, a benchmark for evaluating long-context understanding through the task of determining whether a character's prequel story is consistent with the canonical narrative of the original book.
	Our task poses a stronger demand for global comprehension and deep reasoning than existing benchmarks -- as the prequels are not part of the original story, assessing their plausibility typically requires searching and integrating information that is only indirectly related. Empirically, 88% of instances require evidence from multiple parts of the narrative.
	A comprehensive study on our task demonstrates that:
      </p>
      <ol>
        <li>
          In-context learning, RAG and in-domain training with state-of-the-art LLMs, and commercial DeepResearch services, lag behind humans by >15%;
        </li>
        <li>
          Models often produce correct answers with flawed reasoning, leading to an over 30% gap in reasoning accuracy compared to humans;
        </li>
        <li>
          Recent improvements in LLMs' general reasoning capabilities do not necessarily lead to better long-context reasoning, with a notable performance drop when context is provided;
        </li>
        <li>
	  Combined with the observation that DeepResearch performs poorly on our task, it shows that the task cannot be solved simply by retrieving existing information from the web. Instead, it requires generating new knowledge through reasoning based on learned rules.
        </li>
      </ol>
    </div>

    <center><h1>Definitions of Our Annotation Labels</h1></center>
    <div class="content">
      <center>
        <img src="./img/definitions.png" width="70%" />
        <!-- <figcaption style="text-align: center">
          We refine the labels of <i>Consistent</i> and <i>Contradict</i> with the five cases to ease human annotation. 
        </figcaption> -->
      </center>
    </div>

    <!-- ^ Comparison -->
    <center><h1>Comparison to Existing Story Understanding Benchmarks</h1></center>
    <div class="content">
      <!-- <p>Foo Bar</p> -->
      <p>
	We compare our PRELUDE with existing popular benchmarks on the following essential criteria for long-context understanding and reasoning.
      </p>
      <ol>
        <li>
          <b>Beyond Memorization:</b> LLMs memorize content from pretraining, especially for popular texts, enabling answers without true comprehension. As a <i>Necessity Condition</i>, a robust benchmark must prevent solutions based on memorization alone, ensuring full-context reasoning remains essential.
        </li>
        <li>
          <b>Global Dependency:</b> The task should require aggregating evidence scattered across the context or exhibiting global dependencies; otherwise, it reduces to a short-context problem focused on retrieval rather than true long-text understanding.
        </li>
        <li>
          <b>Depth of Reasoning:</b> Long-context reasoning should inherently require synthesizing multiple pieces of evidence and multi-step deduction, instaed of shallow reasoning, such as decomposition or enumeration. 
        </li>
        <li>
          <b>Human-Machine Gap:</b> To highlight essential capabilities that general-purpose intelligent systems should possess,
a benchmark should show a significant gap between humans and machines.
        </li>
        <li>
          <b>Beyond Summarization/Salience:</b> A strong benchmark should require attention to fine-grained details beyond high-level abstraction to remain challenging and meaningful. Otherwise, it risks reducing to a summarization task that is solvable without long-context understanding.
        </li>
      </ol>
      <center>
        <img src="./img/benchmark_comparison.png" width="70%" />
        <!-- <figcaption style="text-align: center">
          Comparison of existing benchmarks along the different criteria for long context understanding assessment. We report the normalized measure along each criterion.
        </figcaption> -->
      </center>
    </div>
  </body>
</html>
